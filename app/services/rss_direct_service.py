import feedparser
from datetime import datetime
import re
import urllib.request
import urllib.error
from app.models.source import Source
from app.models.category import Category

# Importer le service sp√©cifique pour Les √âchos
from app.services.les_echos_service import get_les_echos_articles

def clean_html(raw_html):
    """Nettoie les tags HTML du texte"""
    if not raw_html:
        return ""
    clean_r = re.compile('<.*?>')
    clean_text = re.sub(clean_r, '', raw_html)
    return clean_text

def fetch_rss_feeds(category_id=None, limit=20):
    """
    R√©cup√®re les articles des flux RSS en direct
    
    Args:
        category_id: ID de la cat√©gorie pour filtrer les sources
        limit: Nombre maximum d'articles √† r√©cup√©rer
    
    Returns:
        Liste d'articles format√©s
    """
    print(f"Appel de fetch_rss_feeds avec category_id={category_id}, limit={limit}")
    
    # R√©cup√©rer les sources appropri√©es
    query = Source.query
    if category_id:
        query = query.filter_by(category_id=category_id)
    sources = query.all()
    
    if not sources:
        print(f"Aucune source trouv√©e pour la cat√©gorie {category_id}")
        return []
    
    all_articles = []
    
    # User-Agent pour √©viter d'√™tre bloqu√©
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    
    source_count = len(sources)
    print(f"Traitement de {source_count} sources")
    
    # R√©cup√©rer la cat√©gorie pour Les √âchos (si filtr√©e)
    category_name = None
    if category_id:
        category = Category.query.get(category_id)
        if category:
            category_name = category.name
    
    # Ajouter les articles des √âchos (en utilisant le service sp√©cial)
    try:
        echos_articles = get_les_echos_articles(category=category_name, limit=5)
        if echos_articles:
            all_articles.extend(echos_articles)
            print(f"Ajout de {len(echos_articles)} articles des √âchos")
    except Exception as e:
        print(f"Erreur lors de la r√©cup√©ration des articles des √âchos: {e}")
    
    # Flux alternatifs pour les sources connues
    alternative_urls = {
        # Le Monde
        "https://www.lemonde.fr/rss/une.xml": "https://www.lemonde.fr/actualite-en-continu/rss_full.xml",
    }
    
    # Traiter les autres sources RSS
    for i, source in enumerate(sources):
        # Ignorer les sources des √âchos (d√©j√† trait√©es)
        if "√âchos" in source.name:
            continue
            
        print(f"Source {i+1}/{source_count}: {source.name} - {source.url}")
        
        try:
            # Utiliser l'URL alternative si disponible
            url = alternative_urls.get(source.url, source.url)
            print(f"  URL utilis√©e: {url}")
            
            # Pr√©paration de la requ√™te
            request = urllib.request.Request(
                url,
                headers={'User-Agent': user_agent}
            )
            
            # R√©cup√©ration du flux
            try:
                with urllib.request.urlopen(request, timeout=10) as response:
                    feed_content = response.read()
                    print(f"  ‚úÖ Flux r√©cup√©r√© ({len(feed_content)} octets)")
            except urllib.error.HTTPError as e:
                print(f"  ‚ùå Erreur HTTP {e.code} pour {url}")
                continue
            except Exception as e:
                print(f"  ‚ùå Erreur lors de la requ√™te: {e}")
                continue
            
            # Analyse du flux
            feed = feedparser.parse(feed_content)
            
            # V√©rifier si des entr√©es sont pr√©sentes
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"  ‚ö†Ô∏è Aucune entr√©e dans le flux")
                continue
            
            # Informations sur le flux
            entry_count = len(feed.entries)
            print(f"  üì∞ {entry_count} articles trouv√©s")
            
            # Traiter les articles
            processed_count = min(entry_count, 5)  # Maximum 5 articles par source
            print(f"  ‚è© Traitement de {processed_count} articles")
            
            # R√©cup√©rer le nom de la cat√©gorie (m√©thode s√©curis√©e)
            try:
                # Essayer d'obtenir la cat√©gorie via la relation
                if hasattr(source, 'category') and source.category:
                    category_name = source.category.name
                else:
                    # Si la relation n'existe pas, chercher directement
                    category = Category.query.get(source.category_id)
                    category_name = category.name if category else "Divers"
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur de r√©cup√©ration de cat√©gorie: {e}")
                category_name = "Divers"
            
            for entry in feed.entries[:processed_count]:
                try:
                    # Titre
                    title = entry.get('title', 'Sans titre')
                    
                    # Lien
                    link = entry.get('link', '')
                    if not link:
                        print(f"  ‚ö†Ô∏è Article sans lien: {title[:30]}...")
                        continue
                    
                    # Description / R√©sum√©
                    summary = ""
                    if hasattr(entry, 'description'):
                        summary = clean_html(entry.description)
                    elif hasattr(entry, 'summary'):
                        summary = clean_html(entry.summary)
                    
                    # Image - PARTIE MODIFI√âE
                    image_url = None
                    
                    # 1. V√©rifier d'abord les enclosures (format FranceInfo)
                    if hasattr(entry, 'enclosures') and entry.enclosures:
                        print(f"  üîç Enclosures trouv√©es ({len(entry.enclosures)})")
                        for enclosure in entry.enclosures:
                            if 'url' in enclosure and enclosure.get('type', '').startswith('image/'):
                                image_url = enclosure['url']
                                print(f"  üñºÔ∏è Image trouv√©e (enclosure): {image_url[:50]}...")
                                break
                    
                    # 2. Si aucune image trouv√©e, essayer media_content
                    if not image_url and hasattr(entry, 'media_content') and entry.media_content:
                        print(f"  üîç Media content trouv√© ({len(entry.media_content)})")
                        for media in entry.media_content:
                            if 'url' in media:
                                image_url = media['url']
                                print(f"  üñºÔ∏è Image trouv√©e (media_content): {image_url[:50]}...")
                                break
                    
                    if not image_url:
                        print(f"  ‚ö†Ô∏è Pas d'image trouv√©e pour: {title[:30]}...")
                    
                    # Date de publication
                    published = datetime.now().strftime('%d/%m/%Y')
                    if hasattr(entry, 'published'):
                        try:
                            # Format standard des flux RSS
                            dt = datetime.strptime(entry.published[:25], '%a, %d %b %Y %H:%M:%S')
                            published = dt.strftime('%d/%m/%Y')
                        except Exception:
                            try:
                                # Format alternatif (avec fuseau horaire)
                                dt = datetime.strptime(entry.published[:25], '%a, %d %b %Y %H:%M:%S %z')
                                published = dt.strftime('%d/%m/%Y')
                            except Exception as e:
                                print(f"  ‚ö†Ô∏è Erreur de parsing de date: {e}")
                    
                    # Cr√©er l'article
                    article = {
                        'title': title,
                        'url': link,
                        'summary': summary[:200] + "..." if len(summary) > 200 else summary,
                        'image_url': image_url,
                        'source_name': source.name,
                        'source_logo': source.logo_url,
                        'published_date': published,
                        'category_name': category_name,
                        'article_direct': True  # Marquer comme article direct
                    }
                    
                    all_articles.append(article)
                    print(f"  ‚úÖ Article ajout√©: {title[:30]}...")
                    
                except Exception as e:
                    print(f"  ‚ùå Erreur traitement article: {e}")
            
        except Exception as e:
            print(f"  ‚ùå Erreur g√©n√©rale: {e}")
    
    article_count = len(all_articles)
    print(f"Total: {article_count} articles r√©cup√©r√©s")
    
    # Tronquer √† la limite demand√©e
    result = all_articles[:limit]
    print(f"Renvoi de {len(result)} articles (limite: {limit})")
    
    return result

def get_categories():
    """
    R√©cup√®re toutes les cat√©gories disponibles
    
    Returns:
        Liste des cat√©gories
    """
    categories = Category.query.all()
    return [{'id': cat.id, 'name': cat.name, 'description': cat.description} for cat in categories]